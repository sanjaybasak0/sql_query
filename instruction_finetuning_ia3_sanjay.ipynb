{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28506e0e-b778-4803-84b1-4c859d2741fd",
      "metadata": {
        "id": "28506e0e-b778-4803-84b1-4c859d2741fd"
      },
      "source": [
        "# Instruction Finetuning using IA3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96",
      "metadata": {
        "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96"
      },
      "source": [
        "In this notebook, we will look into how to perform instruction finetuning using IA3 PEFT method. The task is to perform Supervised finetuning (SFT) of Mistral for Natural language to SQL Query generation task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b098e055-a939-4da7-879e-85849982cdcb",
      "metadata": {
        "id": "b098e055-a939-4da7-879e-85849982cdcb"
      },
      "source": [
        "Load the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
      "metadata": {
        "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
        "outputId": "ad041bef-3f03-4a3d-844b-58ac7b298efd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/transformers/src/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-01-02 21:08:59.358767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 21:08:59.358814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 21:08:59.359638: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 21:08:59.365507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 21:09:00.183017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-01-02 21:09:01,833] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]=\"mistral_instruct_finetuning\"\n",
        "\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from peft import get_peft_model, IA3Config, TaskType\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991163c6-29f3-496e-b71c-f4329ec25df1",
      "metadata": {
        "id": "991163c6-29f3-496e-b71c-f4329ec25df1"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b27033-ba17-4c85-986d-9cbef9262497",
      "metadata": {
        "id": "28b27033-ba17-4c85-986d-9cbef9262497",
        "outputId": "34af3e8e-c21b-4038-f0a9-0cad27a2583e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['content'],\n",
            "        num_rows: 15878\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['content'],\n",
            "        num_rows: 8421\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['content'],\n",
            "        num_rows: 56355\n",
            "    })\n",
            "})\n",
            "{'content': \"Table: 1-1000181-1\\n Columns: ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes']\\n Natural Query: Tell me what the notes are for South Australia \\n SQL Query: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = SOUTH AUSTRALIA</s>\"}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "dataset_name = \"wikisql\"\n",
        "def preprocess(sample):\n",
        "    column_names = sample[\"table\"][\"header\"]\n",
        "    table_id = sample[\"table\"][\"id\"]\n",
        "    natural_query = sample[\"question\"]\n",
        "    sql_query = sample[\"sql\"][\"human_readable\"].replace(\"table\", table_id)\n",
        "    content = f\"Table: {table_id}\\n Columns: {column_names}\\n Natural Query: {natural_query}\\n SQL Query: {sql_query}</s>\"\n",
        "    return {\"content\": content}\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=False,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c195a3-d50d-4fd7-91b3-46f1eef96e44",
      "metadata": {
        "id": "a7c195a3-d50d-4fd7-91b3-46f1eef96e44",
        "outputId": "f822e66f-7669-42cd-bd22-372a2f1648c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table: 1-10007452-3\n",
            " Columns: ['Order Year', 'Manufacturer', 'Model', 'Fleet Series (Quantity)', 'Powertrain (Engine/Transmission)', 'Fuel Propulsion']\n",
            " Natural Query: who is the manufacturer for the order year 1998?\n",
            " SQL Query: SELECT Manufacturer FROM 1-10007452-3 WHERE Order Year = 1998</s>\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"train\"][6][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a",
      "metadata": {
        "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a"
      },
      "source": [
        "## Create the PEFT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641dfc0c-448b-49c4-9643-ce457c0c0ed5",
      "metadata": {
        "id": "641dfc0c-448b-49c4-9643-ce457c0c0ed5"
      },
      "source": [
        "### IA3 Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ae0542-2bc4-47da-8c69-bf122794fcac",
      "metadata": {
        "id": "91ae0542-2bc4-47da-8c69-bf122794fcac"
      },
      "outputs": [],
      "source": [
        "peft_config = IA3Config(target_modules=[\"k_proj\", \"v_proj\", \"down_proj\"],\n",
        "                        feedforward_modules=[\"down_proj\"],\n",
        "                        task_type=TaskType.CAUSAL_LM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d4c86405f4e3418caab58c32a6610746"
          ]
        },
        "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
        "outputId": "ccb8a5fc-ddec-4ad6-f287-89098eebeec8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4c86405f4e3418caab58c32a6610746",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response_template = \"SQL Query:\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = 0\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# cast non-trainable params in bf16\n",
        "for p in model.parameters():\n",
        "    if not p.requires_grad:\n",
        "        p.data = p.to(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8987259-21a6-415d-abcf-7517588b60da",
      "metadata": {
        "id": "d8987259-21a6-415d-abcf-7517588b60da"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78c590d-16cb-4edc-9aab-30b8137a2989",
      "metadata": {
        "id": "a78c590d-16cb-4edc-9aab-30b8137a2989"
      },
      "outputs": [],
      "source": [
        "output_dir = \"mistral_sql_instruct\"\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 8\n",
        "gradient_accumulation_steps = 4\n",
        "logging_steps = 5\n",
        "learning_rate = 5e-4\n",
        "max_grad_norm = 1.0\n",
        "num_train_epochs=1\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 256\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    fp16=True,\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    hub_private_repo=True,\n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
      "metadata": {
        "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
        "outputId": "cbaf776f-2662-4c2d-efed-437a6eab13a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/trl/trl/trainer/sft_trainer.py:282: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"validation\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        "    dataset_text_field=\"content\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b95d1c2-10ba-4c23-9ce9-b1b8ba049313",
      "metadata": {
        "id": "2b95d1c2-10ba-4c23-9ce9-b1b8ba049313",
        "outputId": "47ba7254-0db7-49fb-83e6-a32ac062f452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 524,288 || all params: 7,242,256,384 || trainable%: 0.007239290798352382\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): IA3Model(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1024x1 (cuda:0)])\n",
              "              )\n",
              "              (v_proj): Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1024x1 (cuda:0)])\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(\n",
              "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x14336 (cuda:0)])\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.print_trainable_parameters()\n",
        "trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6c1d33e559e1426ba767965bc5a4f448",
            "b826089c8c054c57be6a004c588f8d79",
            "277fdffc2fa946549465601a4b129a8e",
            "003684ae4ec44ec5b5cfb0efa01a29f8",
            "5fd64eeab8be4ba188be66d91ae80999",
            "4fc66f153620416f8d9a6d3f041affa4",
            "d01b2b9801094b2290f264e5eaeef639",
            "f4d6e8d777fc4933916d4d1e306fdb3b",
            "98af94823551468385fe52f6185d1ae0",
            "d0eb90a362fd441c940f228cecc7cff5",
            "e6c8b3595e58429bac6ca7d0b884b002",
            "d9e370ca1baa442d9904888233723e1b",
            "cbb5587ebd2548199803c77943e08a1d",
            "209c95cde6bb4d97a4c1aa5c4e4931bb",
            "3bdb4122aeca435d8a94e1a74cd83712",
            "f70e680b30ba402abb1e96872db041dd",
            "a8e2b9e616c84f8d8cedde438a70e0d3",
            "47c6e61ef88346adbf948b95c113d6af",
            "9a57dab3b45748eea1024f2a4b82642a",
            "b50f9c2b83394986ab27cf4a9e1ab502",
            "4cff6e6d730746308b285f3945e55654",
            "be19100842484216af971925939c6b8d",
            "b8a1f2c3e8ec41e6b3ad194f5693545d",
            "215920fdbf2345718f228c071d65b645",
            "9ad248cb3bbe439e9513620a0fdb71b0"
          ]
        },
        "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
        "outputId": "9ec47957-10cc-4319-a00f-3169a49b1463"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmangrul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/raid/sourab/temp/wandb/run-20240102_204852-5d5wutxi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/smangrul/mistral_instruct_finetuning/runs/5d5wutxi' target=\"_blank\">exalted-darkness-21</a></strong> to <a href='https://wandb.ai/smangrul/mistral_instruct_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/smangrul/mistral_instruct_finetuning' target=\"_blank\">https://wandb.ai/smangrul/mistral_instruct_finetuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/smangrul/mistral_instruct_finetuning/runs/5d5wutxi' target=\"_blank\">https://wandb.ai/smangrul/mistral_instruct_finetuning/runs/5d5wutxi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='263' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [263/263 14:11, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.092100</td>\n",
              "      <td>0.093658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/trl/trl/trainer/utils.py:127: UserWarning: Could not find response key `SQL Query:` in the following instance: <s> Table: 2-1598533-8\n",
            " Columns: ['Member State sorted by GDP', 'GDP in s billion of USD (2012)', 'GDP % of EU (2012)', 'Annual change % of GDP (2012)', 'GDP per capita in PPP US$ (2012)', 'Public Debt % of GDP (2013 Q1)', 'Deficit (-)/ Surplus (+) % of GDP (2012)', 'Inflation % Annual (2012)', 'Unemp. % 2013 M7']\n",
            " Natural Query: What is the deficit/surplus % of the 2012 GDP of the country with a GDP in billions of USD in 2012 less than 1,352.1, a GDP per capita in PPP US dollars in 2012 greater than 21,615, public debt % of GDP in the 2013 Q1 less than 75.4, and an inflation % annual in 2012 This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/raid/sourab/trl/trl/trainer/utils.py:127: UserWarning: Could not find response key `SQL Query:` in the following instance: <s> Table: 2-14481629-1\n",
            " Columns: ['Year', 'Total Convictions', 'Homicide (Art. 111,112,113,116 StGB)', 'Serious Bodily Injury (Art. 122 StGB)', 'Minor Bodily Injury (Art. 123 StGB)', 'Sexual Contact with Children (Art. 187 StGB)', 'Rape (Art. 190 StGB)', 'Theft (Art. 139 StGB)', 'Robbery (Art. 140 StGB)', 'Receiving Stolen Goods (Art. 160 StGB)', 'Embezzlement (Art. 138 StGB)', 'Fraud (Art. 146 StGB)', 'Narcotics Possession', 'Major Violation of Traffic Laws (Art. 90 SVG)', 'Drunk Driving (Art. 91 Abs. 1 Satz 2 SVG)']\n",
            " Natural Query: What is the total number of Serious This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/raid/sourab/trl/trl/trainer/utils.py:127: UserWarning: Could not find response key `SQL Query:` in the following instance: <s> Table: 2-14481629-1\n",
            " Columns: ['Year', 'Total Convictions', 'Homicide (Art. 111,112,113,116 StGB)', 'Serious Bodily Injury (Art. 122 StGB)', 'Minor Bodily Injury (Art. 123 StGB)', 'Sexual Contact with Children (Art. 187 StGB)', 'Rape (Art. 190 StGB)', 'Theft (Art. 139 StGB)', 'Robbery (Art. 140 StGB)', 'Receiving Stolen Goods (Art. 160 StGB)', 'Embezzlement (Art. 138 StGB)', 'Fraud (Art. 146 StGB)', 'Narcotics Possession', 'Major Violation of Traffic Laws (Art. 90 SVG)', 'Drunk Driving (Art. 91 Abs. 1 Satz 2 SVG)']\n",
            " Natural Query: What is the total number of Rapes This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/raid/sourab/trl/trl/trainer/utils.py:127: UserWarning: Could not find response key `SQL Query:` in the following instance: <s> Table: 2-14481629-1\n",
            " Columns: ['Year', 'Total Convictions', 'Homicide (Art. 111,112,113,116 StGB)', 'Serious Bodily Injury (Art. 122 StGB)', 'Minor Bodily Injury (Art. 123 StGB)', 'Sexual Contact with Children (Art. 187 StGB)', 'Rape (Art. 190 StGB)', 'Theft (Art. 139 StGB)', 'Robbery (Art. 140 StGB)', 'Receiving Stolen Goods (Art. 160 StGB)', 'Embezzlement (Art. 138 StGB)', 'Fraud (Art. 146 StGB)', 'Narcotics Possession', 'Major Violation of Traffic Laws (Art. 90 SVG)', 'Drunk Driving (Art. 91 Abs. 1 Satz 2 SVG)']\n",
            " Natural Query: What is the lowest example of Embe This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n",
            "/raid/sourab/trl/trl/trainer/utils.py:127: UserWarning: Could not find response key `SQL Query:` in the following instance: <s> Table: 2-14481629-1\n",
            " Columns: ['Year', 'Total Convictions', 'Homicide (Art. 111,112,113,116 StGB)', 'Serious Bodily Injury (Art. 122 StGB)', 'Minor Bodily Injury (Art. 123 StGB)', 'Sexual Contact with Children (Art. 187 StGB)', 'Rape (Art. 190 StGB)', 'Theft (Art. 139 StGB)', 'Robbery (Art. 140 StGB)', 'Receiving Stolen Goods (Art. 160 StGB)', 'Embezzlement (Art. 138 StGB)', 'Fraud (Art. 146 StGB)', 'Narcotics Possession', 'Major Violation of Traffic Laws (Art. 90 SVG)', 'Drunk Driving (Art. 91 Abs. 1 Satz 2 SVG)']\n",
            " Natural Query: Which year earlier than 200 This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c1d33e559e1426ba767965bc5a4f448",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 24 LFS files:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b826089c8c054c57be6a004c588f8d79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "277fdffc2fa946549465601a4b129a8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704219097.hf-dgx-01.3854810.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "003684ae4ec44ec5b5cfb0efa01a29f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704219405.hf-dgx-01.3861285.0:   0%|          | 0.00/4.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fd64eeab8be4ba188be66d91ae80999",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704218626.hf-dgx-01.3845745.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fc66f153620416f8d9a6d3f041affa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704219623.hf-dgx-01.3866886.0:   0%|          | 0.00/4.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d01b2b9801094b2290f264e5eaeef639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704219701.hf-dgx-01.3869015.0:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4d6e8d777fc4933916d4d1e306fdb3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704219856.hf-dgx-01.3872307.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98af94823551468385fe52f6185d1ae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704220201.hf-dgx-01.3879288.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0eb90a362fd441c940f228cecc7cff5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704220659.hf-dgx-01.3887871.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6c8b3595e58429bac6ca7d0b884b002",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704221055.hf-dgx-01.3897141.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e370ca1baa442d9904888233723e1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704222313.hf-dgx-01.3923048.0:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbb5587ebd2548199803c77943e08a1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704222492.hf-dgx-01.3928445.0:   0%|          | 0.00/4.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "209c95cde6bb4d97a4c1aa5c4e4931bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704222623.hf-dgx-01.3932057.0:   0%|          | 0.00/5.71k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bdb4122aeca435d8a94e1a74cd83712",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704222906.hf-dgx-01.3938369.0:   0%|          | 0.00/6.63k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f70e680b30ba402abb1e96872db041dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704223183.hf-dgx-01.3943850.0:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8e2b9e616c84f8d8cedde438a70e0d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704223496.hf-dgx-01.3951757.0:   0%|          | 0.00/5.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c6e61ef88346adbf948b95c113d6af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704223703.hf-dgx-01.3956429.0:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a57dab3b45748eea1024f2a4b82642a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704224334.hf-dgx-01.3972500.0:   0%|          | 0.00/8.72k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b50f9c2b83394986ab27cf4a9e1ab502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704224224.hf-dgx-01.3969443.0:   0%|          | 0.00/5.63k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cff6e6d730746308b285f3945e55654",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704224677.hf-dgx-01.3979965.0:   0%|          | 0.00/6.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be19100842484216af971925939c6b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704224815.hf-dgx-01.3983076.0:   0%|          | 0.00/5.93k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8a1f2c3e8ec41e6b3ad194f5693545d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704224931.hf-dgx-01.3985832.0:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "215920fdbf2345718f228c071d65b645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ad248cb3bbe439e9513620a0fdb71b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6a8fc6",
      "metadata": {
        "id": "1d6a8fc6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34be5374-26cc-47f4-bcd6-e48ff5199759",
      "metadata": {
        "id": "34be5374-26cc-47f4-bcd6-e48ff5199759"
      },
      "source": [
        "## Loading the trained model and getting the predictions of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d606954d-4ef9-47b9-8eb3-03f7fabc362e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b559b8d8e3914f12a2ad7a43fee7ffaa"
          ]
        },
        "id": "d606954d-4ef9-47b9-8eb3-03f7fabc362e",
        "outputId": "23031e28-ca09-4a8a-bc76-70967496e497"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/transformers/src/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b559b8d8e3914f12a2ad7a43fee7ffaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): IA3Model(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1024x1 (cuda:0)])\n",
              "              )\n",
              "              (v_proj): Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1024x1 (cuda:0)])\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(\n",
              "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1x14336 (cuda:0)])\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random\n",
        "\n",
        "dataset_name = \"wikisql\"\n",
        "def preprocess(sample):\n",
        "    column_names = sample[\"table\"][\"header\"]\n",
        "    table_id = sample[\"table\"][\"id\"]\n",
        "    natural_query = sample[\"question\"]\n",
        "    sql_query = sample[\"sql\"][\"human_readable\"].replace(\"table\", table_id)\n",
        "    content = f\"Table: {table_id}\\n Columns: {column_names}\\n Natural Query: {natural_query}\\n SQL Query: {sql_query}</s>\"\n",
        "    return {\"content\": content}\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=False,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "peft_model_id = \"Sanjaytfg/mistral_sql_instruct\"\n",
        "device = \"cuda\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.to(torch.float16)\n",
        "model.cuda()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21c47b4-3c50-43ad-905e-1296e196d5a9",
      "metadata": {
        "id": "f21c47b4-3c50-43ad-905e-1296e196d5a9",
        "outputId": "a9a60bda-f97f-4718-9293-00d7234082dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 2-16912096-1\\n Columns: ['Round', 'Pick', 'Player', 'Position', 'School/Club Team']\\n Natural Query: HOW MANY ROUNDS HAD A PICK OF 7?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT COUNT Round FROM 2-16912096-1 WHERE Pick = 7</s>'\n",
            "\n",
            "expected='SELECT COUNT Round FROM 2-16912096-1 WHERE Pick = 7</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 1-14342210-6\\n Columns: ['Player', 'Position', 'Starter', 'Touchdowns', 'Extra points', 'Field goals', 'Points']\\n Natural Query: Which position did Redden play?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Position FROM 1-14342210-6 WHERE Player = Redden</s>'\n",
            "\n",
            "expected='SELECT Position FROM 1-14342210-6 WHERE Player = Redden</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 2-16304749-2\\n Columns: ['South or west terminus', 'North or east terminus', 'First year', 'Final year', 'Notes']\\n Natural Query: What is the most minimal Final year that has a North or east end of covington?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT MIN Final year FROM 2-16304749-2 WHERE North or east terminus = covington</s>'\n",
            "\n",
            "expected='SELECT MIN Final year FROM 2-16304749-2 WHERE North or east terminus = covington</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 2-12017602-24\\n Columns: ['Name', 'Type', 'Local board', 'Suburb', 'Authority', 'Decile']\\n Natural Query: Which name is the learning/social difficulties type?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Name FROM 2-12017602-24 WHERE Type = learning/social difficulties</s>'\n",
            "\n",
            "expected='SELECT Name FROM 2-12017602-24 WHERE Type = learning/social difficulties</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 2-17081606-3\\n Columns: ['Game', 'Date', 'Home Team', 'Result', 'Road Team']\\n Natural Query: What is the Home Team of the game against Seattle on June 1?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Home Team FROM 2-17081606-3 WHERE Date = seattle AND Date = june AND Date = 1</s>'\n",
            "\n",
            "expected='SELECT Home Team FROM 2-17081606-3 WHERE Road Team = seattle AND Date = june 1</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 1-15082102-3\\n Columns: ['Constituency', 'Electorate', 's Spoilt vote', 'Total poll (%)', 'For (%)', 'Against (%)']\\n Natural Query: in electorate of 83850 what is the minimum s split vote\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT MIN s Spoilt vote FROM 1-15082102-3 WHERE Electorate = 83850</s>'\n",
            "\n",
            "expected='SELECT MIN s Spoilt vote FROM 1-15082102-3 WHERE Electorate = 83850</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 2-17116053-1\\n Columns: ['Pick', 'Player', 'Position', 'Nationality', 'Former Team']\\n Natural Query: Which Former Team has a Pick larger than 20?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Former Team FROM 2-17116053-1 WHERE Pick > 20</s>'\n",
            "\n",
            "expected='SELECT Former Team FROM 2-17116053-1 WHERE Pick > 20</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 1-15187735-5\\n Columns: ['Series Ep.', 'Episode', 'Netflix', 'Segment A', 'Segment B', 'Segment C', 'Segment D']\\n Natural Query: When marshmallow cookies is segment b what episode is it on netflix?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Episode FROM 1-15187735-5 WHERE Segment B = marshmallow cookies</s>'\n",
            "\n",
            "expected='SELECT Netflix FROM 1-15187735-5 WHERE Segment B = Marshmallow Cookies</s>'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text=\"Table: 1-14395920-2\\n Columns: ['Stage', 'Winner', 'General classification', 'Points classification', 'Mountains classification', 'Young rider classification', 'Team classification', 'Combativity award']\\n Natural Query: Who won the stage when Mark Cavendish led the points classification, Rinaldo Nocentini led the general classification, and the stage was less than 11.0?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Winner FROM 1-14395920-2 WHERE Points classification = Mark Cavendish AND General classification = Rinaldo Nocentini AND Stage < 11.0</s>'\n",
            "\n",
            "expected='SELECT Winner FROM 1-14395920-2 WHERE Points classification = Mark Cavendish AND General classification = Rinaldo Nocentini AND Stage < 11.0</s>'\n",
            "text=\"Table: 2-15041768-1\\n Columns: ['Title', 'Year', 'Director', 'Budget', 'Gross (worldwide)']\\n Natural Query: What is 2005's budget figure?\\n SQL Query:\"\n",
            "\n",
            "predicted='SELECT Budget FROM 2-15041768-1 WHERE Year = 2005</s>'\n",
            "\n",
            "expected='SELECT Budget FROM 2-15041768-1 WHERE Year = 2005</s>'\n"
          ]
        }
      ],
      "source": [
        "split = \"test\"\n",
        "length = len(dataset[split])\n",
        "for i in range(10):\n",
        "    index = random.randint(0,length)\n",
        "    text = f'{dataset[split][index][\"content\"].split(\"SQL Query:\")[0]}SQL Query:'\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")#, add_special_tokens=False)\n",
        "    inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
        "    with torch.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_new_tokens=128,\n",
        "                                 eos_token_id=tokenizer.eos_token_id)\n",
        "    predicted = tokenizer.decode(outputs[0]).split(\"SQL Query:\")[-1].strip()\n",
        "    expected = dataset[split][index][\"content\"].split(\"SQL Query:\")[-1].strip()\n",
        "\n",
        "    print(f\"{text=}\\n\\n{predicted=}\\n\\n{expected=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7",
      "metadata": {
        "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}